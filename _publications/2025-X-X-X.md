---
title: "Learning the Electronic Structure of Materials at Scale with Distributed Graph Neural Networks"
collection: publications
category: manuscripts
permalink: /publication/2025-X-X-X
excerpt: "<br/><img src='/images/X25.png' style='width: 60%; height: auto;'>>"
date: 2025-04-20
venue: 'International Conference on Learning Representations (ICLR 2026)'
paperurl: 'https://arxiv.org/abs/2507.03840'
# slidesurl: 'https://manasakani.github.io/talks/SC2024'
# citation: 'Your Name, You. (2024). &quot;Paper Title Number 3.&quot; <i>GitHub Journal of Bugs</i>. 1(3).'
---

**Abstract:** Equivariant Graph Neural Networks (eGNNs) trained on density-functional theory (DFT) data can potentially perform electronic structure prediction at unprecedented scales, enabling investigation of the electronic properties of materials with extended defects, interfaces, or exhibiting disordered phases. However, as interactions between atomic orbitals typically extend over 10+ angstroms, the graph representations required for this task tend to be densely connected, and the memory requirements to perform training and inference on these large structures can exceed the limits of modern GPUs. Here we present a distributed eGNN implementation which leverages direct GPU communication and introduce a partitioning strategy of the input graph to reduce the number of embedding exchanges between GPUs. Our implementation shows strong scaling up to 128 GPUs, and weak scaling up to 512 GPUs with 87% parallel efficiency for structures with 3,000 to 190,000 atoms on the Alps supercomputer.


<!-- ![Coverage](https://manasakani.github.io/images/sccrossbar.png) -->
